{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Vowpal Wabbit\n",
    "\n",
    "Vowpal Wabbit (VW) is a general purpose machine learning library which is implementing, among other things, logistic regression with ideas like the hashing trick and per-coordinate adaptive learning rates  (in fact, the hashing trick was made popular by that library). A big advantage  of Vowpal Wabbit is that it is blazing fast. Not only because its underlying implementation is in C++, but also because it is using the L-BFGS optimization method. L-BGFS  stand for  “Limited-memory Broyden–Fletcher–Goldfarb–Shanno” and basically approximates the Broyden–Fletcher–Goldfarb–Shanno ([BFGS](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm)) method using a limited amount of memory.  This method is much more complex to implement than Stochastic Gradient descent (which can be implemented in few lines of code as we saw in our previous post), but is supposedly converging faster (in less iterations). If you want to read more about L-BFGS and/or understand its difference with other optimisation methods, you can check [this](https://github.com/JohnLangford/vowpal_wabbit/wiki/L-BFGS.pdf)  (doc from Vowpal Wabbit) or [this](http://aria42.com/blog/2014/12/understanding-lbfgs) (nice blog post). Note that L-BFGS was empirically observed to be superior to SGD in many cases, in particular in deep learning settings (check out that [paper](http://ai.stanford.edu/~quocle/LeNgiCoaLahProNg11.pdf) on that topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task_data_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"wget {} -O /data/vw_tutorial.zip\".format(task_data_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"unzip /data/vw_tutorial.zip -d /data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4M\t/data/bank-full.csv\r\n",
      "4.0K\t/data/bank-names.txt\r\n",
      "452K\t/data/bank.csv\r\n",
      "0\t/data/bank_train.vw\r\n"
     ]
    }
   ],
   "source": [
    "! du -sh /data/bank*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input format, Namespaces and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset represents the attempt of a bank trying to predict if a marketing phone call will end up in a bank term deposit by the customer, based on a bunch of signals like socio-economic factors of the customer like “does he have a loan?”, etc..\n",
    "\n",
    "\n",
    "\n",
    "The traditional way to represent such datasets is to have a tsv or csv file, with the header being the name of the signals and each line representing the value of the training example on each signal. Each line of the training set has thus a fixed size, and missing values are just a blank cell or some specific value to indicate that it’s missing. Typically, for that dataset, the header looks like that:\n",
    "\n",
    "#### age;job;marital;education;default;balance;housing;loan;contact;day;month;duration;campaign;pdays;previous;poutcome;y\n",
    "\n",
    "With y being the actual supervision (i.e. did the call ended up in bank term deposit). And a typical training example looks like that:\n",
    "\n",
    "#### 58;management;married;tertiary;no;2143;yes;no;unknown;5;may;261;1;-1;0;unknown;no\n",
    "\n",
    "In Vowpal Wabbit, there is no header, and each signal name is embedded in the training example itself. For example, the training example above can look like that in Vowpal Wabbit format:\n",
    "\n",
    "#### -1 |i age:58 balance:2143 duration:261 campaign:1 pdays:-1 previous:0 |c job=management marital=married education=tertiary default=no housing=yes contact=unknown day=5 month=may poutcome=unknown\n",
    "\n",
    "\n",
    "Let’s discuss multiple important things there:\n",
    "\n",
    "* -1 says that this was a negative example.\n",
    "* The |i and |c  are here to specify that the following features are part of a same feature namespace.  Being part of a namespace simply means that all the features in the namespace will be hashed together in a same feature space (this relates to the hashing trick, c.f. the previous post of that series).\n",
    "* Here, i artificially created two namespaces: one for numerical features and another one for categorical ones. But that was just to illustrate the idea of namespace .\n",
    "* In practice, namespaces can be used for different reasons (check the doc here) but one that is particularly useful  is that it allows you to do feature interactions:\n",
    "* For instance, in the command line, using --quadratic ic would combine all the features of the namespaces i and c in our example above to create on the fly 2-way interacting features .  For instance the value of age and job together would be a new signal (maybe if you are a certain age in a certain profession, you’re more or less likely to do a bank term deposit).\n",
    "* Note as well that for the numerical features, i used the colon ‘:‘ and for categorical ones i used ‘=‘ .\n",
    "* Only the  ‘:‘ will be interpreted by Vowpal Wabbit. Both in training and when applying the model, the weight of the corresponding numerical feature (let’s say age) will be multiplied by the actual numerical value in the weighted linear product of the logistic hypothesis (more on that later).\n",
    "* The  ‘=‘ is just cosmetic and for clarity. Technically, writing  married instead of marital=married makes absolutely no difference for the training, except if the value  married could show up in different contexts. E.g. if there were another signal childMarital indicating the marital status of customer’s children,  then you’d have to differentiate if the value married refers to the customer or his children, in which case the feature name would be necessary. Note that if you’d put two such features in different namespaces then they could not be mixed together and the prefix would be again not necessary.\n",
    "* Note that for each signal, i’ve used the full name of the signal as a prefix (e.g. age or marital). First, we just saw that for categorical feature, this is not necessarily  required. For numerical signal though, it is (i.e. you cannot just throw a number without context). Now, for huge training sets, you don’t necessarily want to have a long string repeated millions (or more) of times. A good compromise is to have a mapping between signal names and very short string (like e.g. F1, F2, F3 ….). In the following section, i provide some code that allows to generate such training set with signal names mapping.\n",
    "* There is a nice answer on Quora here exposing a short cheat-sheet  to remind those and how to encode boolean, categorical, ordinal+monotonic or numerical variables in VW.\n",
    "* Last but not least, one thing i love about this format, is that it is very adapted to sparse data. Think that you have thousands of features or maybe just a list of words, then you don’t care about the order of the features or the missing values, you just  throw the features with the right prefix and/or in the right namespace and you’re done. VW will then hash them in their proper bucket in their proper hashing namespace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform your CSV datasets into VW format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/data/bank-full.csv', sep=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation Request:\r\n",
      "  This dataset is public available for research. The details are described in [Moro et al., 2011]. \r\n",
      "  Please include this citation if you plan to use this database:\r\n",
      "\r\n",
      "  [Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \r\n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS.\r\n",
      "\r\n",
      "  Available at: [pdf] http://hdl.handle.net/1822/14838\r\n",
      "                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt\r\n",
      "\r\n",
      "1. Title: Bank Marketing\r\n",
      "\r\n",
      "2. Sources\r\n",
      "   Created by: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012\r\n",
      "   \r\n",
      "3. Past Usage:\r\n",
      "\r\n",
      "  The full dataset was described and analyzed in:\r\n",
      "\r\n",
      "  S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. \r\n",
      "  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, \r\n",
      "  Portugal, October, 2011. EUROSIS.\r\n",
      "\r\n",
      "4. Relevant Information:\r\n",
      "\r\n",
      "   The data is related with direct marketing campaigns of a Portuguese banking institution. \r\n",
      "   The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, \r\n",
      "   in order to access if the product (bank term deposit) would be (or not) subscribed. \r\n",
      "\r\n",
      "   There are two datasets: \r\n",
      "      1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010).\r\n",
      "      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.\r\n",
      "   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).\r\n",
      "\r\n",
      "   The classification goal is to predict if the client will subscribe a term deposit (variable y).\r\n",
      "\r\n",
      "5. Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)\r\n",
      "\r\n",
      "6. Number of Attributes: 16 + output attribute.\r\n",
      "\r\n",
      "7. Attribute information:\r\n",
      "\r\n",
      "   For more information, read [Moro et al., 2011].\r\n",
      "\r\n",
      "   Input variables:\r\n",
      "   # bank client data:\r\n",
      "   1 - age (numeric)\r\n",
      "   2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\r\n",
      "                                       \"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\") \r\n",
      "   3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\r\n",
      "   4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\r\n",
      "   5 - default: has credit in default? (binary: \"yes\",\"no\")\r\n",
      "   6 - balance: average yearly balance, in euros (numeric) \r\n",
      "   7 - housing: has housing loan? (binary: \"yes\",\"no\")\r\n",
      "   8 - loan: has personal loan? (binary: \"yes\",\"no\")\r\n",
      "   # related with the last contact of the current campaign:\r\n",
      "   9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\") \r\n",
      "  10 - day: last contact day of the month (numeric)\r\n",
      "  11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", ..., \"nov\", \"dec\")\r\n",
      "  12 - duration: last contact duration, in seconds (numeric)\r\n",
      "   # other attributes:\r\n",
      "  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\r\n",
      "  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\r\n",
      "  15 - previous: number of contacts performed before this campaign and for this client (numeric)\r\n",
      "  16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\r\n",
      "\r\n",
      "  Output variable (desired target):\r\n",
      "  17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\r\n",
      "\r\n",
      "8. Missing Attribute Values: None\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/data/bank-names.txt', 'r') as text:\n",
    "    print text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = list(data)\n",
    "#write numerical and categorical features names\n",
    "num_features = #YOUR CODE HERE\n",
    "cat_features = set(header) ^ set(num_features) ^ set('y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, data_test= train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Replace 'no' with -1 and 'yes with 1\n",
    "\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vw_file(filename, dataset):\n",
    "    # write function which would write dataset to file in vowpal wabbit format\n",
    "    \n",
    "    #YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_vw_file('bank_train.vw', data_train)\n",
    "create_vw_file('bank_test.vw', data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by a first command to train a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model.vw\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = bank_train.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.416214   0.416214            3         3.0  -1.0000  -0.7298       16\n",
      "0.235059   0.053904            6         6.0  -1.0000  -1.0000       17\n",
      "0.491115   0.798382           11        11.0   1.0000  -0.9921       18\n",
      "0.643804   0.796493           22        22.0  -1.0000  -0.2591       18\n",
      "0.502135   0.360465           44        44.0  -1.0000  -1.0000       17\n",
      "0.377984   0.250946           87        87.0  -1.0000  -1.0000       17\n",
      "0.377113   0.376242          174       174.0  -1.0000  -1.0000       17\n",
      "0.388875   0.400637          348       348.0   1.0000   0.0729       18\n",
      "0.391542   0.394209          696       696.0  -1.0000  -1.0000       17\n",
      "0.371666   0.351790         1392      1392.0  -1.0000  -0.8303       17\n",
      "0.344147   0.316628         2784      2784.0  -1.0000  -1.0000       17\n",
      "0.321426   0.298705         5568      5568.0  -1.0000  -0.9416       16\n",
      "0.322568   0.323710        11135     11135.0  -1.0000  -0.8868       17\n",
      "0.306410   0.290251        22269     22269.0  -1.0000  -1.0000       17\n",
      "\n",
      "finished run\n",
      "number of examples = 31647\n",
      "weighted example sum = 31647\n",
      "weighted label sum = -24265\n",
      "average loss = 0.300657\n",
      "best constant = -0.766739\n",
      "total feature number = 541323\n"
     ]
    }
   ],
   "source": [
    "!vw bank_train.vw -f model.vw --loss_function squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty much self explained:\n",
    "* -f is to specify the filename of the output mode and\n",
    "* --loss_function specifies which loss function to use, squared in our case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to actually use the model on a separate test set you simply do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "Num weight bits = 18\n",
      "learning rate = 10\n",
      "initial_t = 1\n",
      "power_t = 0.5\n",
      "predictions = preds.txt\n",
      "using no cache\n",
      "Reading datafile = bank_test.vw\n",
      "num sources = 1\n",
      "average    since         example     example  current  current  current\n",
      "loss       last          counter      weight    label  predict features\n",
      "0.069848   0.069848            3         3.0  -1.0000  -0.5437       17\n",
      "0.089859   0.109869            6         6.0  -1.0000  -0.8352       17\n",
      "0.320497   0.597264           11        11.0   1.0000  -0.5074       17\n",
      "0.401549   0.482601           22        22.0   1.0000   0.1998       17\n",
      "0.295090   0.188632           44        44.0  -1.0000  -1.0000       17\n",
      "0.372497   0.451703           87        87.0   1.0000  -0.5474       17\n",
      "0.415658   0.458819          174       174.0  -1.0000  -1.0000       17\n",
      "0.349235   0.282813          348       348.0  -1.0000  -1.0000       18\n",
      "0.326170   0.303104          696       696.0   1.0000  -0.9686       17\n",
      "0.296370   0.266571         1392      1392.0  -1.0000  -1.0000       17\n",
      "0.301250   0.306130         2784      2784.0  -1.0000  -1.0000       17\n",
      "0.285543   0.269835         5568      5568.0   1.0000  -0.8099       17\n",
      "0.291492   0.297442        11135     11135.0  -1.0000  -1.0000       17\n",
      "\n",
      "finished run\n",
      "number of examples = 13564\n",
      "weighted example sum = 13564\n",
      "weighted label sum = -10368\n",
      "average loss = 0.288757\n",
      "best constant = -0.764506\n",
      "total feature number = 232004\n"
     ]
    }
   ],
   "source": [
    "!vw bank_test.vw -t -i model.vw -p preds.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some options i found useful and interesting for the training part:\n",
    "\n",
    "* -c --passes N .  This specifies to do N passes on the training set while learning the optimal weights. In deep learning, the term epoch is often used instead of pass, and basically represents a full pass over the whole training set to update the weights. Doing several passes often leads to stronger models but the ideal number of passes can be tuned as an hyper parameter.  Note that the  -c option specifying to use caching is necessary when doing multiple passes because from the second pass, VW is using pre-compiled information that it prepared/cached during the first pass.\n",
    "* -b N  . The -b option allows you to control the number of bits in the hashing namespace (c.f part 2 of this series to understand what is the hashing trick ) and set it to 2N . The default value for N is 18, which might be more than ok (e.g. for the toy bank dataset) or not enough depending on the cardinality of your features values. If you need to encode  features having an high cardinality, i.e. a lot of different values like e.g. a product id in a catalog of millions of product, or, more frequently, if you need to create interactions of features (i.e. the cartesian product of two features values) which is also often leading to an high cardinality features, then you’ll probably need to increase N. Obviously the higher it is, the less collisions you’ll have in your namespace, but the more memory you’ll need.\n",
    "* --interactions arg . This is a very powerful one. Basically  arg is a list of letters, and each letter represents a namespace (assuming you organised your features around namespaces, like e.g. in our example in previous section). Applying that option means that it will automatically create interactions between all features in the corresponding namespaces. For instance, in our example above, adding e.g.  --interactions ic   will instantly create a whole bunch of new features in the model: all the interactions pairs between features in the namespace i and in the namespace c . Note that in this case the option is equivalent to --quadratic ic but the --interactions option is more general as it allows to create not only quadratic interactions but even more (triplets, quadruplets etc…). Such a feature somehow allows you to get closer to factorization machine models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902321099812\n"
     ]
    }
   ],
   "source": [
    "preds = pd.read_csv('preds.txt', header=None)\n",
    "test_split = pd.read_csv('bank_test.vw', header=None, sep = '|')\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(test_split[0].values, preds[0].values)\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at other [arguments](https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments) and try to achive better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
