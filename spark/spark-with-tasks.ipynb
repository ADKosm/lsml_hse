{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Resilient Distributed Datasets\n",
    "\n",
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\n",
    "\n",
    "Formally, an RDD is a read-only, partitioned collection of records. RDDs can be created through deterministic operations on either data on stable storage or other RDDs. RDD is a fault-tolerant collection of elements that can be operated on in parallel.\n",
    "\n",
    "There are two ways to create RDDs − parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system, such as a shared file system, HDFS, HBase, or any data source offering a Hadoop Input Format.\n",
    "\n",
    "Spark makes use of the concept of RDD to achieve faster and efficient MapReduce operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setting up SparkContext\n",
    "SparkContext (aka Spark context) is the heart of a Spark application.\n",
    "\n",
    "You could also assume that a SparkContext instance is a Spark application.\n",
    "\n",
    "Spark context sets up internal services and establishes a connection to a Spark execution environment.\n",
    "\n",
    "Once a SparkContext is created you can use it to create RDDs, accumulators and broadcast variables, access Spark services and run jobs (until SparkContext is stopped).\n",
    "\n",
    "A Spark context is essentially a client of Spark’s execution environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/hdp/current/spark2.1\"\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python'))\n",
    "sys.path.insert(0, os.path.join(os.environ[\"SPARK_HOME\"], 'python/lib/py4j-0.10.4-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sparkConf = pyspark.SparkConf() \\\n",
    "    .set(\"spark.executor.memory\", \"2560m\")\\\n",
    "    .set(\"spark.driver.memory\", \"2560m\")\\\n",
    "    .set(\"spark.yarn.executor.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.yarn.driver.memoryOverhead\", 3584)\\\n",
    "    .set(\"spark.python.worker.memory\", \"1536m\")\\\n",
    "    .set(\"spark.executor.instances\", 11)\\\n",
    "    .set(\"spark.default.parallelism\", 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Other configuration properties can be found [here](https://spark.apache.org/docs/latest/configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(\n",
    "    master='yarn-client',\n",
    "    appName='seminar3-rdd',\n",
    "    conf=sparkConf\n",
    ")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Web UI (aka Application UI or webUI or Spark UI) is the web interface of a running Spark application to monitor and inspect Spark job executions in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "port = sc.uiWebUrl.split(':')[-1]\n",
    "print 'http://cluster1:{}'.format(port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Getting the Data Files\n",
    "\n",
    "In this notebook, we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file that we will download locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The KDD Cup 1999 competition dataset is described in detail \n",
    "[here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! wget \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\" -O \"/data/kddcup.data_10_percent.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Put data into hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put /data/kddcup.data_10_percent.gz ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Creating a RDD from a File\n",
    "The most common way of creating an RDD is to load it from a file. Notice that Spark's textFile can handle compressed files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = 'kddcup.data_10_percent.gz'\n",
    "raw_data = sc.textFile(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have our data file loaded into the raw_data RDD.\n",
    "\n",
    "Without getting into Spark transformations and actions, the most basic thing we can do to check that we got our RDD contents right is to count() the number of lines loaded from the file into the RDD and check a few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another way of creating an RDD is to parallelize an already existing list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd_list = sc.parallelize([x + 5 for x in range(100)])\n",
    "print rdd_list.count()\n",
    "rdd_list.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RDD Basic Operations\n",
    "This section will introduce three basic but essential Spark operations. Two of them are the transformations map and filter. The other is the action collect. At the same time we will introduce the concept of persistence in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The filter Transformation\n",
    "This transformation can be applied to RDDs in order to keep just elements that satisfy a certain condition. More concretely, a functions is evaluated on every element in the original RDD. The new resulting RDD will contain just those elements that make the function return True.\n",
    "\n",
    "For example, imagine we want to count how many normal. interactions we have in our dataset. We can filter our raw_data RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "normal_raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The map Transformation\n",
    "By using the map transformation in Spark, we can apply a function to every element in our RDD. Python's lambdas are specially expressive for this particular.\n",
    "\n",
    "In this case we want to read our data file as a CSV formatted one. We can do this by applying a lambda function to each element in the RDD as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "csv_data.take(1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### FlatMap transformation\n",
    "By using flatMap you can map each row to multiple new rows. Like in word count example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = sc.parallelize(['Of course we can use predefined functions with map and not just lambda',\n",
    "                       'Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. normal) and the value is the whole list of elements that represents the row in the CSV formatted file', \n",
    "                       'We could proceed as follows'])\n",
    "words = texts.flatMap(lambda x: x.lower().split(' '))\n",
    "words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Using map to create PairRDD\n",
    "If you have a tuple of length 2 as your RDD data type, you can use \\*ByKey operations on your RDD, with first value of tuple being the key and second being the value. Let's create such RDD.\n",
    "\n",
    "Of course we can use predefined functions with map and not just lambda. Imagine we want to have each element in the RDD as a key-value pair where the key is the tag (e.g. normal) and the value is the whole list of elements that represents the row in the CSV formatted file. We could proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "\n",
    "key_csv_data = raw_data.map(parse_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can change key with standard map function. Let's say we want to aggregate data by tag and protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def protocol_key(x):\n",
    "    tag = x[0]\n",
    "    proto = x[1][1]\n",
    "    return '{}_{}'.format(tag, proto), 1\n",
    "\n",
    "type_protocol = key_csv_data.map(protocol_key)\n",
    "protocols_by_type = dict(type_protocol.reduceByKey(lambda x, y: x + y).collect())\n",
    "protocols_by_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Antother way to acheive this is to use groupBy functions. In this case we get iterable with values corresponding to each key as second tuple value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_by = key_csv_data.groupByKey()\n",
    "grouped_by.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And then we can map values to desired statistic. Write a function that will get us same results as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def protocol_counter(values):\n",
    "    # Task 1\n",
    "    pass\n",
    "\n",
    "assert protocol_counter([(0, 'udp'), (0, 'udp'), (0, 'tcp')])['udp'] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "protocols_by_type2 = dict(grouped_by.mapValues(protocol_counter).collect())\n",
    "assert protocols_by_type2['normal.']['udp'] == protocols_by_type['normal._udp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# DataFrame API\n",
    "DataFrame is another Spark API which is very convinient for structured data.\n",
    "\n",
    "To use it, we need to instantiate a SparkSession, which is essentialy just enhaced SparkContext.\n",
    "\n",
    "In our case we can construct it directly from SparkContext, but if don't have one already, we can create session via builder, almost the same as with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A DataFrame is a Dataset organized into named columns. \n",
    "It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: csv, structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "To create one we utilize a DataFrameReader avaliable in SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = ss.read.csv(data_path)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sometimes it's more convinient to use pandas dataframe representation in notebooks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We don't have column names in our data, but they are avaliable seperatley. Let's rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "header = requests.get('http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names').text.split('\\n')[1:-1]\n",
    "\n",
    "types = [h.split(':')[1].strip(' .') for h in header]\n",
    "header = [h.split(':')[0] for h in header]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_header = data\n",
    "for i, h in enumerate(header + ['tag']):\n",
    "    data_with_header = data_with_header.withColumnRenamed('_c{}'.format(i), h)\n",
    "data_with_header.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DataFrames have schema - information about columns in dataframe. You can view it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "All of the columns have string type - thats because we read them from csv and didn't use inferSchema flag. Lets cast continuous columns ourselves.\n",
    "\n",
    "To do this, we use spark column functions. \n",
    "Column represents a column in a Dataset that holds a Catalyst Expression that produces a value per row.\n",
    "You can construct Column insatance from it's name using pyspark.sql.functions.col and then call different functions on it, including cast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cast_if_continuous(col_name, t):\n",
    "    if t == u'continuous':\n",
    "        return sf.col(col_name).cast('float').alias(h)\n",
    "    else:\n",
    "        return sf.col(col_name)\n",
    "\n",
    "data_with_types = data_with_header.select([cast_if_continuous(h, t) for h, t in zip(header, types)] + ['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have apropriate types in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_with_types.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You also can do different transformations on columns. For example let's calculate mean error rate for each column.\n",
    "\n",
    "There are several ways to introduce new column into our dataframe.\n",
    "One of them is to use .withColumn, which accepts column expression and column name.\n",
    "\n",
    "Another is to use .select with different column expressions as arguments.\n",
    "Expressions also could be strings or constants, which internally transforms to columns using sf.col or sf.lit (literal value).\n",
    "To provide a name for new column, you can call .alias on column.\n",
    "\n",
    "You can use '\\*' wildcard to select all columns in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_er_df = data_with_types.select('tag', sf.col('protocol_type'), \n",
    "                         ((sf.col('dst_host_serror_rate') + \n",
    "                           sf.col('dst_host_srv_serror_rate') +\n",
    "                           sf.col('dst_host_rerror_rate') + \n",
    "                           sf.col('dst_host_srv_rerror_rate') / 4).alias('mean_err_rate')))\n",
    "mean_er_df.orderBy('mean_err_rate', ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "It's a lot easier to do aggregations on data using DataFrame API, because sf module also provides so called aggregate functions, which can be used with .groupby.\n",
    "\n",
    "Let's calculate the same statistic as in RDD API\n",
    "\n",
    "First, group data by two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grouped_df = data_with_types.groupBy('tag', 'protocol_type')\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, aggregate it with corresponding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pt_df = grouped_df.agg(sf.count('protocol_type').alias('count'))\n",
    "pt_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "protocols_by_type3 = {'{}_{}'.format(r['tag'], r['protocol_type']):r['count'] for r in pt_df.collect()}\n",
    "assert protocols_by_type3['normal._tcp'] == protocols_by_type['normal._tcp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As an exercise, calculate mean size (scr_bytes column) of payload for each tag. List of aggregate functions can be found [here](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "mean_src_bytes_by_tag_df = ...\n",
    "mean_src_bytes_by_tag = ...\n",
    "assert mean_src_bytes_by_tag['teardrop.'] == 28"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
