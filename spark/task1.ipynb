{"nbformat_minor": 2, "cells": [{"source": "# Fix all jupyter notebook problems\n\n<sub> 0. Pray </sub> \n1. Connect to head machine via SSH\n2. Open `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` and change 5 to 4.\n3. Fix anaconda installation via official fix script. \n```\ncurl https://gregorysfixes.blob.core.windows.net/public/fix-conda.sh | sudo sh\n```\n4. Install all necessary python packages. At least kaggle - \n```\nsudo /usr/bin/anaconda/bin/conda install -c conda-forge kaggle --yes\n```\n5. Open Ambari and restart jupyter service.\n6. Open azure jupyter notebook and upload this notebook\n7. Check, that cells below can be executed correctly", "cell_type": "markdown", "metadata": {}}, {"source": "# Create Spark Context", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "sc = spark.sparkContext", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1582190762543_0008</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-spark2.nqjiyvt1lw2u5puwbzzkpomikc.bx.internal.cloudapp.net:8088/proxy/application_1582190762543_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn2-spark2.nqjiyvt1lw2u5puwbzzkpomikc.bx.internal.cloudapp.net:30060/node/containerlogs/container_1582190762543_0008_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 47.552978515625, "end_time": 1582237561341.93}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "import pandas as pd\nfrom pyspark.sql import SparkSession\n\nss = SparkSession(sc)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 241.994873046875, "end_time": 1582237561595.04}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "hadoop = sc._jvm.org.apache.hadoop\nfs = hadoop.fs.FileSystem\nconf = hadoop.conf.Configuration() \npath = hadoop.fs.Path('/')\n    \ndef hdfs_ls(path):\n    result = []\n    for f in fs.get(conf).listStatus(hadoop.fs.Path(path)):\n        result.append(str(f.getPath()))\n    return result", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 246.215087890625, "end_time": 1582237561859.174}}, "collapsed": false}}, {"execution_count": 5, "cell_type": "code", "source": "from pprint import pprint\n\npprint(hdfs_ls('/'))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/custom-scriptaction-logs',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/app-logs',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/tmp',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/warehouse',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mapred',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HDInsight_TestAccessiblityBlobName',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hive',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/example',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/apps',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiSamples',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/yarn',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiNotebooks',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/amshbase',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/ams',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/atshistory',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hbase',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hdp',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/user',\n 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mr-history']"}], "metadata": {"cell_status": {"execute_time": {"duration": 758.401123046875, "end_time": 1582237603500.463}}, "collapsed": false}}, {"source": "# Download task data\n\nDownload data directly from kaggle. Read this to understand how: https://github.com/Kaggle/kaggle-api", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! cat ~/.kaggle/kaggle.json", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 210.26806640625, "end_time": 1582219662750.281}}, "collapsed": false}}, {"execution_count": 65, "cell_type": "code", "source": "%%local\n! kaggle competitions files outbrain-click-prediction", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/spark/.kaggle/kaggle.json'\nname                           size  creationDate         \n----------------------------  -----  -------------------  \nclicks_test.csv.zip           135MB  2018-06-22 05:33:10  \ndocuments_entities.csv.zip    126MB  2018-06-22 05:33:10  \ndocuments_topics.csv.zip      121MB  2018-06-22 05:33:10  \ndocuments_categories.csv.zip   32MB  2018-06-22 05:33:10  \npage_views_sample.csv.zip     149MB  2018-06-22 05:33:10  \nclicks_train.csv.zip          390MB  2018-06-22 05:33:10  \npage_views.csv.zip             35GB  2018-06-22 05:33:10  \nevents.csv.zip                478MB  2018-06-22 05:33:10  \nsample_submission.csv.zip     100MB  2018-06-22 05:33:10  \npromoted_content.csv.zip        3MB  2018-06-22 05:33:10  \ndocuments_meta.csv.zip         16MB  2018-06-22 05:33:10  \n"}], "metadata": {"cell_status": {"execute_time": {"duration": 1054.541015625, "end_time": 1582220036186.809}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! kaggle competitions download outbrain-click-prediction", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 1029.8291015625, "end_time": 1582237621450.165}}, "collapsed": false}}, {"source": "# Load data to HDFS", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "https://www.kaggle.com/c/outbrain-click-prediction/data", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 15, "cell_type": "code", "source": "%%local\n! hdfs dfs -rm -r /task1\n! hdfs dfs -mkdir /task1", "outputs": [{"output_type": "stream", "name": "stdout", "text": "20/02/20 22:32:48 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Delete operation as thread count 0 is <= 1\n20/02/20 22:32:48 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Delete operation is: 37 ms with threads: 0\nDeleted /task1\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 4507.58203125, "end_time": 1582237971017.23}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! for i in `ls *.zip`; do unzip -p $i | tqdm | hadoop fs -put - /task1/${i//\\.zip/}; done", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5347.8388671875, "end_time": 1582237943122.283}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hadoop fs -du -s -h /task1/*.csv", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2246.673095703125, "end_time": 1582237727913.001}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# Read example", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf.dtypes", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\npvdf.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 76 ms, sys: 20 ms, total: 96 ms\nWall time: 9min 16s\n"}, {"execution_count": 15, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Parquet is faster than CSV", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\npvdf.write.parquet(\"/task1/page_views.parquet\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 31, "cell_type": "code", "source": "%%local\n! hadoop fs -du -s -h /task1/page_views.parquet", "outputs": [{"output_type": "stream", "name": "stdout", "text": "47.3 G  /task1/page_views.parquet\r\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 33, "cell_type": "code", "source": "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 35, "cell_type": "code", "source": "%%time\nfrom IPython.display import display\nboo = pvdf2.groupBy(\"geo_location\").count().collect()\ndisplay(boo[:5])", "outputs": [{"output_type": "display_data", "data": {}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\nWall time: 22.2 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 36, "cell_type": "code", "source": "%%time\nboo = pvdf.groupBy(\"geo_location\").count().collect()\ndisplay(boo[:5])", "outputs": [{"output_type": "display_data", "data": {}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "CPU times: user 96 ms, sys: 20 ms, total: 116 ms\nWall time: 10min 34s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Convert all to Parquet", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 58, "cell_type": "code", "source": "%%time\ndef convert_all_to_parquet():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        if fn.endswith(\".csv\"):\n            fn_after = fn.replace(\".csv\", \".parquet\")\n            path_before = fn\n            path_after = fn_after\n            if fn_after not in all_files:\n                # generate parquet\n                df = ss.read.csv(path_before, header=True)\n                df.write.parquet(path_after)\n            print(fn_after, \"done\")\n\nconvert_all_to_parquet()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "clicks_test.parquet done\nclicks_train.parquet done\ndocuments_categories.parquet done\ndocuments_entities.parquet done\ndocuments_meta.parquet done\ndocuments_topics.parquet done\nevents.parquet done\npage_views.parquet done\npage_views_sample.parquet done\npromoted_content.parquet done\nsample_submission.parquet done\nCPU times: user 96 ms, sys: 0 ns, total: 96 ms\nWall time: 4min 37s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "Remove csv, we have parquet now", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hdfs dfs -rm /task1/*.csv", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 60, "cell_type": "code", "source": "%%local\n! hadoop fs -du -s -h /task1/*", "outputs": [{"output_type": "stream", "name": "stdout", "text": "133.2 M  /task1/clicks_test.parquet\n367.5 M  /task1/clicks_train.parquet\n36.5 M  /task1/documents_categories.parquet\n184.0 M  /task1/documents_entities.parquet\n21.2 M  /task1/documents_meta.parquet\n183.3 M  /task1/documents_topics.parquet\n669.3 M  /task1/events.parquet\n47.3 G  /task1/page_views.parquet\n236.9 M  /task1/page_views_sample.parquet\n5.0 M  /task1/promoted_content.parquet\n184.2 M  /task1/sample_submission.parquet\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Preview all files", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 65, "cell_type": "code", "source": "%%time\ndef preview_all_files():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        df = ss.read.parquet(fn)\n        print(\"#\" * 15 + \" {0} \".format(fn) + \"#\" * 15)\n        df.show(1)\n        \npreview_all_files()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "############### /task1/clicks_test.parquet ###############\n+----------+------+\n|display_id| ad_id|\n+----------+------+\n|  17805143|288388|\n+----------+------+\nonly showing top 1 row\n\n############### /task1/clicks_train.parquet ###############\n+----------+-----+-------+\n|display_id|ad_id|clicked|\n+----------+-----+-------+\n|         1|42337|      0|\n+----------+-----+-------+\nonly showing top 1 row\n\n############### /task1/documents_categories.parquet ###############\n+-----------+-----------+----------------+\n|document_id|category_id|confidence_level|\n+-----------+-----------+----------------+\n|    1544588|       1513|     0.263546236|\n+-----------+-----------+----------------+\nonly showing top 1 row\n\n############### /task1/documents_entities.parquet ###############\n+-----------+--------------------+-----------------+\n|document_id|           entity_id| confidence_level|\n+-----------+--------------------+-----------------+\n|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n+-----------+--------------------+-----------------+\nonly showing top 1 row\n\n############### /task1/documents_meta.parquet ###############\n+-----------+---------+------------+-------------------+\n|document_id|source_id|publisher_id|       publish_time|\n+-----------+---------+------------+-------------------+\n|     325048|      822|         253|2013-02-27 00:00:00|\n+-----------+---------+------------+-------------------+\nonly showing top 1 row\n\n############### /task1/documents_topics.parquet ###############\n+-----------+--------+------------------+\n|document_id|topic_id|  confidence_level|\n+-----------+--------+------------------+\n|     801028|     280|0.0148711250868194|\n+-----------+--------+------------------+\nonly showing top 1 row\n\n############### /task1/events.parquet ###############\n+----------+--------------+-----------+---------+--------+------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|\n+----------+--------------+-----------+---------+--------+------------+\n|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n+----------+--------------+-----------+---------+--------+------------+\nonly showing top 1 row\n\n############### /task1/page_views.parquet ###############\n+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 1 row\n\n############### /task1/page_views_sample.parquet ###############\n+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 1 row\n\n############### /task1/promoted_content.parquet ###############\n+-----+-----------+-----------+-------------+\n|ad_id|document_id|campaign_id|advertiser_id|\n+-----+-----------+-----------+-------------+\n|    1|       6614|          1|            7|\n+-----+-----------+-----------+-------------+\nonly showing top 1 row\n\n############### /task1/sample_submission.parquet ###############\n+----------+--------------------+\n|display_id|               ad_id|\n+----------+--------------------+\n|  21960532|50582 190398 2293...|\n+----------+--------------------+\nonly showing top 1 row\n\nCPU times: user 28 ms, sys: 4 ms, total: 32 ms\nWall time: 2.47 s\n"}], "metadata": {"scrolled": false, "collapsed": false, "editable": true, "deletable": true}}, {"source": "# Register all tables to be usable in SQL queries", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "%%time\ndef register_all_tables():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        if fn.endswith(\".parquet\"):\n            table_name = fn.replace(\".parquet\", \"\")\n            df = ss.read.parquet(fn)\n            df.registerTempTable(table_name)\n            print(table_name, \"done\")\n        \nregister_all_tables()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "clicks_test done\nclicks_train done\ndocuments_categories done\ndocuments_categories2 done\ndocuments_entities done\ndocuments_meta done\ndocuments_topics done\nevents done\nevents_test done\nevents_train done\njoined_events done\npage_views done\npage_views_sample done\npromoted_content done\nsample_submission done\nCPU times: user 36 ms, sys: 8 ms, total: 44 ms\nWall time: 18.2 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# SQL query example", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 72, "cell_type": "code", "source": "%%time\nss.sql(\"\"\"\nselect count(distinct(uuid)) as users_count\nfrom events\n\"\"\").collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 16.1 s\n"}, {"execution_count": 72, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# 1. Baseline", "cell_type": "markdown", "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "Simple model using the following features:\n- **clicked**\n- geo_location features (country, state, dma)\n- day_of_week (from timestamp, use *date.isoweekday()*)\n- ad_id\n- ad_document_id\n- campaign_id\n- advertiser_id\n- display_document_id\n- platform", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## Calculate features for VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "- Use DataFrame API to join tables (functions in SQL queries: https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html)\n- Use Python API to calculate features and save them as text for VW (*saveAsTextFile()*)\n- Hash features in Spark (24 bits, use *sklearn.utils.murmurhash.murmurhash3_32*)\n- Split dataset in Spark into 90% train, 10% test **by display_id**, save the split for further use", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "from sklearn.utils.murmurhash import murmurhash3_32\ndef hasher(x, bits):\n    return murmurhash3_32(x) % 2**bits", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5286.1611328125, "end_time": 1582238139803.682}}, "editable": true, "collapsed": true, "deletable": true}}, {"source": "## Copy data from HDFS to cluster machine", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "We will run vowpal wabbit **locally**, need to copy data from HDFS", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hdfs dfs -getmerge /task1/train.txt train.txt", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hdfs dfs -getmerge /task1/test.txt test.txt", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Install VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "Connect to head node via SSH and install vw:\n\n```\ngit clone --recursive https://github.com/VowpalWabbit/vowpal_wabbit.git\nsudo apt install libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev zlib1g-dev cmake g++ -y\ncd vowpal_wabbit && make && cd build && make install\necho \"export PATH=/usr/local/bin:\\$PATH\" >> ~/.bashrc\n```\n\nOr you can download prebuilded vw \n```\nmkdir -p bin/\nwget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O bin/vw\nchmod +x bin/vw\n```", "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "%%local\n! wget http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624 -O vw\n! chmod +x vw", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2020-02-20 22:16:52--  http://finance.yendor.com/ML/VW/Binaries/vw-8.20190624\nResolving finance.yendor.com (finance.yendor.com)... 69.163.152.190\nConnecting to finance.yendor.com (finance.yendor.com)|69.163.152.190|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9467376 (9.0M)\nSaving to: \u2018vw\u2019\n\nvw                  100%[===================>]   9.03M  13.7MB/s    in 0.7s    \n\n2020-02-20 22:16:53 (13.7 MB/s) - \u2018vw\u2019 saved [9467376/9467376]\n\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 1194.341796875, "end_time": 1582237013877.734}}, "collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "%%local\n! ./vw --help | head", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Num weight bits = 18\r\nlearning rate = 0.5\r\ninitial_t = 0\r\npower_t = 0.5\r\nusing no cache\r\nReading datafile = \r\nnum sources = 1\r\ndriver:\r\n  --onethread           Disable parse thread\r\nVW options:\r\n  --ring_size arg (=256, ) size of example ring\r\n  --strict_parse           throw on malformed examples\r\nUpdate options:\r\n  -l [ --learning_rate ] arg Set learning rate\r\n  --power_t arg              t power value\r\n  --decay_learning_rate arg  Set Decay factor for learning_rate between passes\r\n  --initial_t arg            initial t value\r\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 210.831787109375, "end_time": 1582237053042.234}}, "collapsed": false}}, {"source": "## Train VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 40, "cell_type": "code", "source": "%%local\n! head -n2 train.txt", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1 |f 1748258:1.0 1376507:1.0 11602085:1.0 14125547:1.0 15617356:1.0 16621393:1.0 4204498:1.0 9609462:1.0 5728439:1.0 11418299:1.0\r\n-1 |f 5611969:1.0 11418299:1.0 11602085:1.0 15617356:1.0 15750989:1.0 786735:1.0 3083696:1.0 4204498:1.0 5728439:1.0 1376507:1.0\r\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 204.3740234375, "end_time": 1582237145215.502}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 41, "cell_type": "code", "source": "%%local\n! LD_LIBRARY_PATH=/usr/local/lib ./vw -d train.txt -b 24 -c -k --ftrl --passes 1 -f model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "final_regressor = model\r\nEnabling FTRL based optimization\r\nAlgorithm used: Proximal-FTRL\r\nftrl_alpha = 0.005\r\nftrl_beta = 0.1\r\nNum weight bits = 24\r\nlearning rate = 0.5\r\ninitial_t = 0\r\npower_t = 0.5\r\ncreating cache_file = train.txt.cache\r\nReading datafile = train.txt\r\nnum sources = 1\r\naverage  since         example        example  current  current  current\r\nloss     last          counter         weight    label  predict features\r\n\r\nfinished run\r\nnumber of examples = 2\r\nweighted example sum = 2.000000\r\nweighted label sum = 0.000000\r\naverage loss = 0.700492\r\nbest constant = 0.000000\r\nbest constant's loss = 0.693147\r\ntotal feature number = 22\r\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 312.14306640625, "end_time": 1582237147864.006}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "## Check VW test performance", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 55, "cell_type": "code", "source": "%%local\n! LD_LIBRARY_PATH=/usr/local/lib ./vw -d test.txt -i model -t -k -p test_predictions.txt --progress 1000000 --link=logistic", "outputs": [{"output_type": "stream", "name": "stdout", "text": "only testing\r\npredictions = test_predictions.txt\r\nEnabling FTRL based optimization\r\nAlgorithm used: Proximal-FTRL\r\nftrl_alpha = 0.005\r\nftrl_beta = 0.1\r\nNum weight bits = 24\r\nlearning rate = 0.5\r\ninitial_t = 0\r\npower_t = 0.5\r\nusing no cache\r\nReading datafile = test.txt\r\nnum sources = 1\r\naverage  since         example        example  current  current  current\r\nloss     last          counter         weight    label  predict features\r\n\r\nfinished run\r\nnumber of examples = 2\r\nweighted example sum = 2.000000\r\nweighted label sum = 0.000000\r\naverage loss = 0.966958\r\nbest constant = 0.000000\r\nbest constant's loss = 1.000000\r\ntotal feature number = 22\r\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 214.2021484375, "end_time": 1582237318373.902}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 64, "cell_type": "code", "source": "%%local\n! cat test_predictions.txt | head -n2", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.505993\r\n0.497650\r\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 207.689208984375, "end_time": 1582237359437.642}}, "collapsed": false}}, {"execution_count": 57, "cell_type": "code", "source": "%%local\nimport numpy as np\n\ndef read_vw_predictions(p):\n    y_pred = []\n    with open(p, \"r\") as f:\n        for line in f:\n            y_pred.append(float(line.split()[0]))\n    return np.array(y_pred)\n\ny_pred = read_vw_predictions(\"test_predictions.txt\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 7.95703125, "end_time": 1582237326519.143}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 58, "cell_type": "code", "source": "%%local\ndef get_vw_y_true(p):\n    y_true = []\n    with open(p, \"r\") as f:\n        for line in f:\n            y_true.append(float(line.partition(\" \")[0]))\n    return np.array(y_true)\n\ny_true = get_vw_y_true(\"test.txt\")", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 5.323974609375, "end_time": 1582237326796.148}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 59, "cell_type": "code", "source": "%%local\nfrom sklearn.metrics import log_loss, roc_auc_score", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2.138916015625, "end_time": 1582237328727.368}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\nprint(y_pred)\nlog_loss(y_true, y_pred)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 6.97900390625, "end_time": 1582237329208.456}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\nroc_auc_score(y_true, y_pred)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 8.0791015625, "end_time": 1582237338761.081}}, "editable": true, "collapsed": false, "deletable": true}}, {"source": "# 2. Better model", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "This time let's make a personalized recommender using:\n- page views information\n- document properties\n\nIdeas for features:\n- uuid topic, entity, publisher, ... preferences\n- document similarities\n- ...", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## More SQL examples", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "# we start with a DataFrame\nevents_df = ss.sql(\"select * from events\")\nevents_df.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+--------------+-----------+---------+--------+------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|\n+----------+--------------+-----------+---------+--------+------------+\n|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n+----------+--------------+-----------+---------+--------+------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 23, "cell_type": "code", "source": "# we can make RDD of Rows with *.rdd\nfrom pyspark.sql import Row\nevents_df.rdd.take(3)", "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 30, "cell_type": "code", "source": "# When it's RDD, we can use Python to create new RDD of Rows\n(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n).take(3)", "outputs": [{"execution_count": 30, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 31, "cell_type": "code", "source": "# we can convert it back to DataFrame if it's still a table that can be converted to Java types\nss.createDataFrame(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n).show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+-------------+\n|           bar|          foo|\n+--------------+-------------+\n|cb8c55702adb93|[US, SC, 519]|\n|79a85fa78311b9|[US, CA, 807]|\n|822932ce3d8757|[US, MI, 505]|\n+--------------+-------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 36, "cell_type": "code", "source": "%%time\n# we can save it to HDFS as parquet (if it's a DataFrame)\nss.createDataFrame(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n).write.mode(\"overwrite\").parquet(\"/task1/example1\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\nWall time: 3min 4s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 39, "cell_type": "code", "source": "ss.read.parquet(\"/task1/example1\").printSchema()\nss.read.parquet(\"/task1/example1\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- bar: string (nullable = true)\n |-- foo: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+--------------+-------------+\n|           bar|          foo|\n+--------------+-------------+\n|cb8c55702adb93|[US, SC, 519]|\n|79a85fa78311b9|[US, CA, 807]|\n|822932ce3d8757|[US, MI, 505]|\n+--------------+-------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 41, "cell_type": "code", "source": "%%time\n# or we can skip DataFrame API if we use Python functions (there will be no speed increase)\n(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n).saveAsPickleFile(\"/task1/example2\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\nWall time: 2min 41s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 43, "cell_type": "code", "source": "sc.pickleFile(\"/task1/example2\").take(3)", "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 49, "cell_type": "code", "source": "# sometimes we cannot make a DataFrame\nimport numpy as np\nrdd = (\n    events_df.rdd\n    .map(lambda x: Row(x=np.array(x.geo_location.split(\">\") if x.geo_location else [])))\n)\nrdd.take(2)", "outputs": [{"execution_count": 49, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "# throws TypeError: not supported type: <type 'numpy.ndarray'>\nss.createDataFrame(rdd)", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 52, "cell_type": "code", "source": "%%time\n# but we can save as RDD in pickle file just fine\nrdd.saveAsPickleFile(\"/task1/example3\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\nWall time: 3min 31s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "Takeaways:\n- use DataFrames when you can (simple join's, select's, groupby's), it will be faster\n- use RDD and Python when you can't use DataFrame API\n- convert it back to DataFrame if needed\n- or save to pickles (can save almost any Python object as pickle)", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "# Built-in SQL functions", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "You can find more at https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 53, "cell_type": "code", "source": "# sql version\ndf = ss.sql(\"\"\"\nselect\n    document_id,\n    collect_list(struct(category_id, confidence_level)) as categories\nfrom\n    documents_categories\ngroup by document_id\n\"\"\")\ndf.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+--------------------+\n|document_id|          categories|\n+-----------+--------------------+\n|     100010|[[1513,0.79842798...|\n|    1000240|[[1505,0.92], [15...|\n|    1000280|[[1909,0.92], [19...|\n+-----------+--------------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 55, "cell_type": "code", "source": "%%time\ndf.write.mode(\"overwrite\").parquet(\"/task1/example4\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 12.5 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 57, "cell_type": "code", "source": "# or we can use RDD and Python if we are not aware of those SQL functions\nrdd = (\n    ss.sql(\"select * from documents_categories\")\n    .rdd\n    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n    .groupByKey()\n    .map(lambda (k, vs): (k, list(vs)))\n)\nrdd.take(3)", "outputs": [{"execution_count": 57, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 58, "cell_type": "code", "source": "%%time\n# it's much slower, but we can do almost everything in Python\nrdd.saveAsPickleFile(\"/task1/example5\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 28 ms, sys: 12 ms, total: 40 ms\nWall time: 3min 31s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 63, "cell_type": "code", "source": "# but sometimes with Python we can do more\nrdd = (\n    ss.sql(\"select * from documents_categories\")\n    .rdd\n    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n    .groupByKey()\n    .map(lambda (k, vs): Row(document_id=k, categories={a: float(b) for a, b in vs}))\n)", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 60, "cell_type": "code", "source": "%%time\n# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\nss.createDataFrame(rdd).write.parquet(\"/task1/example6\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\nWall time: 25.8 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 64, "cell_type": "code", "source": "ss.read.parquet(\"/task1/example6\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----------+\n|          categories|document_id|\n+--------------------+-----------+\n|Map(1510 -> 0.887...|    1059269|\n|Map(1408 -> 0.92,...|    1050604|\n|Map(1903 -> 0.92,...|    1472688|\n+--------------------+-----------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 65, "cell_type": "code", "source": "# now we can join this table with events for instance\nss.read.parquet(\"/task1/example6\").registerTempTable(\"doc_categories_ready\")", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 68, "cell_type": "code", "source": "ss.sql(\"\"\"\nselect \n    e.*, \n    dc.categories\nfrom \n    events as e\n    join doc_categories_ready as dc on dc.document_id = e.document_id\n\"\"\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+--------------+-----------+---------+--------+------------+--------------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|          categories|\n+----------+--------------+-----------+---------+--------+------------+--------------------+\n|  18242074|e703634e3dfa39|    1000240|536236046|       2|          NG|Map(1503 -> 0.07,...|\n|  18694427|5b023d28c0a9f3|    1000240|687121504|       2|   US>MA>521|Map(1503 -> 0.07,...|\n|   3436070|55e1db49ff4eef|    1000240|223783698|       1|   US>CA>803|Map(1503 -> 0.07,...|\n+----------+--------------+-----------+---------+--------+------------+--------------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}
